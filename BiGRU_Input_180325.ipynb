{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-57f909873e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/rita/Google Drive/630/project/model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Code'"
     ]
    }
   ],
   "source": [
    "import os, re, sys, time, json, codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from inspect import getargspec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.chdir(\"/Users/rita/Google Drive/630/project/model\")\n",
    "\n",
    "from Code.Input_functions import *\n",
    "\n",
    "% matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "EMBEDDING_WORD = 'Embedding/GoogleNews-vectors-negative300.bin'\n",
    "EMBEDDING_CHAR = 'Embedding/char2vecLearned25'\n",
    "TRAIN_DATA_FILE = 'Dataset/questions/training'\n",
    "TRAIN_DATA_FILE = 'Dataset/questions/training'\n",
    "VAL_DATA_FILE = 'Dataset/cnn/questions/validation'\n",
    "TEST_DATA_FILE = 'Dataset/cnn/questions/test'\n",
    "\n",
    "N_TRAININGPOINTS = 10000\n",
    "if N_TRAININGPOINTS == 10000:\n",
    "    MAX_NUM_WORDS, MAX_NUM_CHARS = 16319, 66 # total 49465, 112\n",
    "elif N_TRAININGPOINTS == 50000:\n",
    "    MAX_NUM_WORDS, MAX_NUM_CHARS = 26751, 67 # total 71640, 181\n",
    "    \n",
    "MAX_SEQUENCE_LENGTH_NEWS = 700 # median ~ 700\n",
    "MAX_SEQUENCE_LENGTH_QUES = 37 # max ~ 37\n",
    "EMBEDDING_DIM_WORD = 300\n",
    "EMBEDDING_DIM_CHAR = 25\n",
    "\n",
    "UNK_WORD = \"<UNK_WORD>\"\n",
    "UNK_CHAR = \"^\"\n",
    "UNK_ENTITY = \"<UNK_ENTITY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_dataset(data_file, name, first=100000000, remove_stopwords=False, stem_words=False, remove_punc=False, keep_period=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Finished 1000 questions in train\n",
      "Finished 2000 questions in train\n",
      "Finished 3000 questions in train\n",
      "Finished 4000 questions in train\n",
      "Finished 5000 questions in train\n",
      "Finished 6000 questions in train\n",
      "Finished 7000 questions in train\n",
      "Finished 8000 questions in train\n",
      "Finished 9000 questions in train\n",
      "Finished 10000 questions in train\n",
      "Finished 1000 questions in val\n",
      "Finished 2000 questions in val\n",
      "Finished 3000 questions in val\n",
      "Finished 1000 questions in test\n",
      "Finished 2000 questions in test\n",
      "Finished 3000 questions in test\n",
      "Found 10001 questions in trainset\n",
      "Found 3924 questions in valset\n",
      "Found 3198 questions in testset\n",
      "26.795522928237915 sec\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Loading datasets\")\n",
    "'''\n",
    "datasets = {\"news\":[], \"questions\":[], \"answers\":[]}\n",
    "entities = [(news, questions, answer, entities)]\n",
    "'''\n",
    "entities = defaultdict(list)\n",
    "trainsets, entities[\"train\"] = load_dataset(TRAIN_DATA_FILE, \"train\", N_TRAININGPOINTS, remove_stopwords=False, stem_words=False, remove_punc=False)\n",
    "valsets, entities[\"val\"] = load_dataset(VAL_DATA_FILE, \"val\", remove_stopwords=False, stem_words=False, remove_punc=False)\n",
    "testsets, entities[\"test\"] = load_dataset(TEST_DATA_FILE, \"test\", remove_stopwords=False, stem_words=False, remove_punc=False)\n",
    "print(\"Found {} questions in trainset\".format(len(trainsets[\"answers\"]))) # 380298\n",
    "print(\"Found {} questions in valset\".format(len(valsets[\"answers\"]))) # 3924\n",
    "print(\"Found {} questions in testset\".format(len(testsets[\"answers\"]))) # 3198\n",
    "\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 545sec for all, 68sec for 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.94% of trainsets have answers\n",
      "Total unique tokens in the trainset: 71640\n",
      "Total unique chars in the trainset: 181\n",
      "Median news length: 699.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(trainsets['news'])):\n",
    "    if trainsets[\"answers\"][i] in trainsets[\"news\"][i].split()[:MAX_SEQUENCE_LENGTH_NEWS]:\n",
    "        count += 1\n",
    "print(\"{0:.2f}% of trainsets have answers\".format(count/len(trainsets['news'])*100))\n",
    "print(\"Total unique tokens in the trainset: {}\".format(len(Counter([j for i in trainsets[\"news\"] for j in i.split()]))))\n",
    "print(\"Total unique chars in the trainset: {}\".format(len(Counter([k for i in trainsets[\"news\"] for j in i.split() for k in j]))))\n",
    "print(\"Median news length: {}\".format(np.median([len(trainsets[\"news\"][i].split()) for i in range(len(trainsets['news']))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property experts say @placeholder investment in @entity5 is set to grow\n",
      "( @entity0 ) sophisticated , glamorous and spacious - - when the super - rich go house - hunting they are searching for something special . real estate in @entity5 swankier suburbs can catch a buyers eye . @entity8 , @entity9 and @entity10 have long been the stomping ground of the elite - - and are now welcoming a new wave of @entity15 investors . \" the @entity19 who are coming into @entity5 now are @entity19 who themselves have worked for their money , \" explains @entity16 , a @entity18 - @entity17 wealth manager based in @entity5 . \" they have grown in industry and are actually part of the exciting story of the @entity15 renaissance , \" she continues . \" it bringing to @entity5 the best of the continent . \" these investors are having a considerable impact on @entity5 property market and they mainly come from just six countries : @entity17 , @entity32 , @entity33 , @entity34 , @entity35 and @entity36 . of these , @entity17 are splashing out the most cash when it comes to bricks and mortar in the @entity18 capital - - typically spending between $ 22 and $ 37 million on securing a property , according to luxury property agents @entity44 . their research shows that over the past three years @entity19 have spent over $ 900 million on luxury residential property in @entity5 . \" the new international @entity15 is very well - traveled , \" explains @entity16 . \" educated in the @entity47 , @entity18 and different parts of @entity49 their taste is definitely more modern and clean . \" @entity52 owning a home in post codes like @entity55 or @entity55 - - around the corner from @entity57 - - means more than having a place to lay your head . these buildings are investments which are expected to gain even bigger value in the coming years . high - end auction house @entity64 says that foreign investors see @entity5 as a \" safe haven \" for prime property investments , and ranks the city as the second most important hub for ultra high - net - worth homes . the only spot more important on the planet is @entity74 . for evidence that @entity5 still attracts high - end buyers , look no further than the sale of a penthouse in @entity8 which fetched $ 40 million earlier this year . educated thinking as well as an intelligent investment , many of the @entity15 buyers see these houses as a way of maintaining long standing cultural ties with @entity5 - - and it here they want to send their children to school . @entity87 , @entity88 , @entity89 are all among the list of respected institutions that teach the offspring of wealthy @entity19 . the @entity17 @entity93 in @entity5 calculates that @entity17 nationals now spend over $ 446 million per year on fees , tutoring and accommodation at @entity18 schools and university . \" @entity15 clients are very much driven by the need to educate their children , \" says @entity16 . \" education usually means putting the children on an international stage , and that one reason why this is feeding into the demand for property in @entity5 . \" indeed , education industry experts @entity111 say there were over 17 , 500 @entity17 studying in @entity18 universities in 2012 - - about 1 , 000 more than the 2009 / 10 academic session . and experts are expecting this trend to continue . \" virtually all the transactions are for end use , not rental investment , which indicates that the @entity15 buyer market in @entity5 has significant room for growth , \" says @entity117 , director at @entity44 . \" african buyers or luxury tenants in @entity5 are currently where the @entity126 and @entity127 were five years ago . they have the resources and desire to purchase or rental luxury homes in @entity5 , \" he adds . \" it is going to be the @entity15 century . \" more from @entity133 read this : @entity133 green lean speed machines read this : @entity15 designs rocking art world editor note : @entity141 covers the macro trends impacting the region and also focuses on the continent key industries and corporations\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(testsets[\"questions\"][0])\n",
    "print(testsets[\"news\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word-level Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Handle OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( @entity0 ) - - each summer , @entity1 <UNK_WORD> a <UNK_WORD> of <UNK_WORD> and shoppers with eager cash <UNK_WORD> . before jumping into the peak - season pig pile , consider the advantages of an o\n"
     ]
    }
   ],
   "source": [
    "handeling_oov = False\n",
    "\n",
    "if handeling_oov == True:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Replacing OOV')\n",
    "\n",
    "    texts = trainsets[\"news\"] + trainsets[\"questions\"]\n",
    "    word_counts_all = Counter([j for i in texts for j in i.split()])\n",
    "    notOOV = word_counts_all.most_common(MAX_NUM_WORDS)\n",
    "    OOV = word_counts_all - Counter(dict(notOOV))\n",
    "    if \"|\" in OOV:\n",
    "        OOV = OOV - Counter({\"|\":OOV[\"|\"]})\n",
    "    texts = [re.sub(r\"( {} )\".format(\" | \".join(list(OOV.keys()))), \" {} \".format(UNK_WORD), text) for text in texts]\n",
    "    np.save(\"Dataset/GRU/{0}/traintext{0}_OOV.npy\".format(N_TRAININGPOINTS), texts)\n",
    "    \n",
    "    print(\"{} sec\".format(time.time() - start_time)) # 1600sec x 3\n",
    "\n",
    "else:\n",
    "    texts = np.load(\"Dataset/GRU/{0}/traintext{0}_OOV.npy\".format(N_TRAININGPOINTS)).tolist()\n",
    "    print(texts[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Texts to Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming texts to sequences - Word Level\n",
      "Found 16319 unique tokens\n",
      "Median News Length: 696.0\n",
      "Max Question Length: 37\n",
      "14.099263906478882 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Transforming texts to sequences - Word Level')\n",
    "\n",
    "tokenizer_word = Tokenizer(filters='', lower=False, split=\" \", char_level=False)\n",
    "tokenizer_word.fit_on_texts(texts)\n",
    "\n",
    "trainSeqNews_word = tokenizer_word.texts_to_sequences(trainsets[\"news\"])\n",
    "trainSeqQues_word = tokenizer_word.texts_to_sequences(trainsets[\"questions\"])\n",
    "\n",
    "valSeqNews_word = tokenizer_word.texts_to_sequences(valsets[\"news\"])\n",
    "valSeqQues_word = tokenizer_word.texts_to_sequences(valsets[\"questions\"])\n",
    "\n",
    "testSeqNews_word = tokenizer_word.texts_to_sequences(testsets[\"news\"])\n",
    "testSeqQues_word = tokenizer_word.texts_to_sequences(testsets[\"questions\"])\n",
    "\n",
    "word_counts = tokenizer_word.word_counts\n",
    "word_index = tokenizer_word.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "print(\"Median News Length: {}\".format(np.median(np.array([len(i.split()) for i in trainsets[\"news\"]] + [len(i.split()) for i in valsets[\"news\"]]))))\n",
    "print(\"Max Question Length: {}\".format(np.max(np.array([len(i.split()) for i in trainsets[\"questions\"]] + [len(i.split()) for i in valsets[\"questions\"]]))))\n",
    "\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 60sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n",
      "60.838651180267334 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Indexing word vectors')\n",
    "\n",
    "word2vecGooNews = KeyedVectors.load_word2vec_format(EMBEDDING_WORD, binary=True) # a word:vec dictionary\n",
    "# word2vec.save_word2vec_format('googlenews.txt')\n",
    "print('Found {} word vectors of word2vec'.format(len(word2vecGooNews.vocab)))\n",
    "\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 60sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Prepare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix - word\n",
      "Null word embeddings: 1059\n",
      "Embedding shape: (16320, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix - word')\n",
    "\n",
    "nb_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "\n",
    "embedding_word_matrix = np.zeros((nb_words, EMBEDDING_DIM_WORD))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        break\n",
    "    if word in word2vecGooNews.vocab:\n",
    "        embedding_word_matrix[i] = word2vecGooNews.word_vec(word)\n",
    "print('Null word embeddings: {}'.format(np.sum(np.sum(embedding_word_matrix, axis=1) == 0)))\n",
    "print('Embedding shape: {}'.format(embedding_word_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Char-level Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Texts to Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming texts to sequences - Character Level\n",
      "Found 66 unique tokens\n",
      "55.71695685386658 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Transforming texts to sequences - Character Level')\n",
    "\n",
    "texts = trainsets[\"news\"] + trainsets[\"questions\"]\n",
    "texts = [re.sub(r\"[^A-z0-9 \\\"\\'\\.\\?\\{\\}\\(\\)\\[\\]:;!~@#$%&*<>,/+\\-=_]\", \"^\", text) for text in texts]\n",
    "\n",
    "tokenizer_char = Tokenizer(filters='', lower=False, split=\" \", char_level=True)\n",
    "tokenizer_char.fit_on_texts(texts)\n",
    "\n",
    "trainSeqNews_char = tokenizer_char.texts_to_sequences(trainsets[\"news\"])\n",
    "trainSeqQues_char = tokenizer_char.texts_to_sequences(trainsets[\"questions\"])\n",
    "\n",
    "valSeqNews_char = tokenizer_char.texts_to_sequences(valsets[\"news\"])\n",
    "valSeqQues_char = tokenizer_char.texts_to_sequences(valsets[\"questions\"])\n",
    "\n",
    "testSeqNews_char = tokenizer_char.texts_to_sequences(testsets[\"news\"])\n",
    "testSeqQues_char = tokenizer_char.texts_to_sequences(testsets[\"questions\"])\n",
    "\n",
    "char_index = tokenizer_char.word_index\n",
    "char_counts = tokenizer_char.word_counts\n",
    "print('Found {} unique tokens'.format(len(char_index)))\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 109sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Generate / Load Char Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67 word vectors of word2vec\n",
      "3.7756259441375732 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "chars = [list(re.sub(r\"[^A-z0-9 \\\"\\'\\.\\?\\{\\}\\(\\)\\[\\]:;!~@#$%&*<>,/+\\-=_]\", \"^\", text)) for text in texts]\n",
    "# char2vecLearned = Word2Vec(chars, size=EMBEDDING_DIM_CHAR, min_count=1)\n",
    "# char2vecLearned.save(EMBEDDING_CHAR)\n",
    "char2vecLearned = Word2Vec.load(EMBEDDING_CHAR).wv\n",
    "print('Found {} word vectors of word2vec'.format(len(char2vecLearned.vocab)))\n",
    "\n",
    "print(\"{} sec\".format(time.time() - start_time)) # 201sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Prepare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix - char\n",
      "Null char embeddings: 2\n",
      "Embedding shape: (67, 25)\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix - char')\n",
    "\n",
    "nb_chars = min(MAX_NUM_CHARS, len(char_index)) + 1\n",
    "\n",
    "embedding_char_matrix = np.zeros((nb_chars, EMBEDDING_DIM_CHAR))\n",
    "for char, i in char_index.items():\n",
    "    if i >= MAX_NUM_CHARS:\n",
    "        break\n",
    "    if char in char2vecLearned.vocab:\n",
    "        embedding_char_matrix[i] = char2vecLearned.word_vec(char)\n",
    "print('Null char embeddings: {}'.format(np.sum(np.sum(embedding_char_matrix, axis=1) == 0)))\n",
    "print('Embedding shape: {}'.format(embedding_char_matrix.shape))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Input and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Pad Sequences as Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding sequences\n",
      "Shape of news tensor: (10001, 700)\n",
      "Shape of questions tensor: (10001, 37)\n"
     ]
    }
   ],
   "source": [
    "print('Padding sequences')\n",
    "\n",
    "News_train_word = pad_sequences(trainSeqNews_word, maxlen=MAX_SEQUENCE_LENGTH_NEWS, truncating=\"post\")\n",
    "Ques_train_word = pad_sequences(trainSeqQues_word, maxlen=MAX_SEQUENCE_LENGTH_QUES, truncating=\"post\")\n",
    "print('Shape of news tensor:', News_train_word.shape)\n",
    "print('Shape of questions tensor:', Ques_train_word.shape)\n",
    "\n",
    "News_val_word = pad_sequences(valSeqNews_word, maxlen=MAX_SEQUENCE_LENGTH_NEWS, truncating=\"post\")\n",
    "Ques_val_word = pad_sequences(valSeqQues_word, maxlen=MAX_SEQUENCE_LENGTH_QUES, truncating=\"post\")\n",
    "\n",
    "News_test_word = pad_sequences(testSeqNews_word, maxlen=MAX_SEQUENCE_LENGTH_NEWS, truncating=\"post\")\n",
    "Ques_test_word = pad_sequences(testSeqQues_word, maxlen=MAX_SEQUENCE_LENGTH_QUES, truncating=\"post\")\n",
    "\n",
    "np.save(\"Dataset/GRU/{0}/N_train{0}.npy\".format(N_TRAININGPOINTS), News_train_word)\n",
    "np.save(\"Dataset/GRU/{0}/Q_train{0}.npy\".format(N_TRAININGPOINTS), Ques_train_word)\n",
    "np.save(\"Dataset/GRU/{0}/N_val{0}.npy\".format(N_TRAININGPOINTS), News_val_word)\n",
    "np.save(\"Dataset/GRU/{0}/Q_val{0}.npy\".format(N_TRAININGPOINTS), Ques_val_word)\n",
    "np.save(\"Dataset/GRU/{0}/N_test{0}.npy\".format(N_TRAININGPOINTS), News_test_word)\n",
    "np.save(\"Dataset/GRU/{0}/Q_test{0}.npy\".format(N_TRAININGPOINTS), Ques_test_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Input Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine input embedding\n",
      "Null word embeddings: 2\n",
      "Embedding shape: (16320, 325)\n"
     ]
    }
   ],
   "source": [
    "print('Combine input embedding')\n",
    "\n",
    "num_words = len(word_index) + 1\n",
    "sorted_word_index = sorted(word_index.items(), key=lambda x:x[1])\n",
    "embedding_input_matrix = np.zeros((num_words, EMBEDDING_DIM_WORD + EMBEDDING_DIM_CHAR))\n",
    "for wordcode in range(1, num_words):\n",
    "    \n",
    "    if wordcode == word_index[UNK_WORD]:\n",
    "        continue\n",
    "    \n",
    "    wordorig = sorted_word_index[wordcode-1][0]\n",
    "    \n",
    "    # get word embedding\n",
    "    try:\n",
    "        word_level_embedding = embedding_word_matrix[wordcode]\n",
    "    except:\n",
    "        word_level_embedding = embedding_word_matrix[word_index[UNK_WORD]] # (300,)\n",
    "    \n",
    "    # get char embedding\n",
    "    char_level_embedding =[]\n",
    "    for charorig in wordorig:\n",
    "        try:\n",
    "            charcode = char_index[charorig]\n",
    "        except:\n",
    "            charcode = char_index[UNK_CHAR]\n",
    "        char_level_embedding.append(embedding_char_matrix[charcode])\n",
    "    char_level_embedding = np.mean(np.array(char_level_embedding), axis=0) # (25,)\n",
    "    \n",
    "    # combine word and char embedding\n",
    "    embedding_input_matrix[wordcode] = np.concatenate((word_level_embedding, char_level_embedding)) # (325,)\n",
    "\n",
    "print('Null word embeddings: {}'.format(np.sum(np.sum(embedding_input_matrix, axis=1) == 0)))\n",
    "print('Embedding shape: {}'.format(embedding_input_matrix.shape))\n",
    "np.save(\"Dataset/GRU/{0}/embedding_input_matrix{0}.npy\".format(N_TRAININGPOINTS), embedding_input_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Output Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found unique entity: 386\n",
      "y_train labels: 10001\n",
      "y_val labels: 3924\n",
      "y_test labels: 3198\n"
     ]
    }
   ],
   "source": [
    "entity_index = sorted(list(set([w for w in word_index.keys() if w.startswith('@entity')] + trainsets[\"answers\"])), key=lambda x:int(x[7:])) + [UNK_ENTITY]\n",
    "entity_index = {w: index for (index, w) in enumerate(entity_index)}\n",
    "print('Found unique entity: {}'.format(len(entity_index)))\n",
    "np.save(\"Dataset/GRU/{0}/entity_index{0}.npy\".format(N_TRAININGPOINTS), entity_index)\n",
    "\n",
    "y_train = np.array([entity_index[trainsets[\"answers\"][i]] for i in range(len(trainsets[\"answers\"]))])\n",
    "y_val = np.array([entity_index[valsets[\"answers\"][i]] for i in range(len(valsets[\"answers\"]))])\n",
    "y_test = np.array([entity_index[testsets[\"answers\"][i]] for i in range(len(testsets[\"answers\"]))])\n",
    "print('y_train labels: {}'.format(len(y_train)))\n",
    "print('y_val labels: {}'.format(len(y_val)))\n",
    "print('y_test labels: {}'.format(len(y_test)))\n",
    "np.save(\"Dataset/GRU/{0}/y_train{0}.npy\".format(N_TRAININGPOINTS), y_train)\n",
    "np.save(\"Dataset/GRU/{0}/y_val{0}.npy\".format(N_TRAININGPOINTS), y_val)\n",
    "np.save(\"Dataset/GRU/{0}/y_test{0}.npy\".format(N_TRAININGPOINTS), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
